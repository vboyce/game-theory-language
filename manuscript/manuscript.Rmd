---
title: "Game theory and language: a cautionary tale"
authors:
  - name: Veronica Boyce
    department: Department of Psychology
    affiliation: Stanford University
    email: vboyce@stanford.edu
  - name: Avidit Acharya
    department: Department of Political Science
    affiliation: Stanford University
  - name: Michael C. Frank
    department: Department of Psychology
    affiliation: Stanford University
abstract: |
  An attempt was made TODO
bibliography: references.bib
biblio-style: unsrt
output: 
  rticles::arxiv_article:
    extra_dependencies: ["float"]

---

# Introduction

How do we use language to coordinate with each other?

The use of language and the formation of linguistic conventions in pure reference games is studied frequently, but the role of language is studied less in other situations with more complicated goals that may not be fully aligned between participants. 

In prior work, we tried to look at the role of language in a 3-player negotiation game where incentives were either fully-aligned or competitive between players. We found some evidence that language use improved performance, and that the referential language used to pick out targets showed some signs of conventionalization over time. However, this was a game we had created, so there were not theoretical or analytic predictions about what the optimal strategies were either with or without language, and therefore, when or how communication (via language) might improve performance. 

Here, we attempted to bridge that gap, by looking at the role of language in two classic game theory games. By using set-ups that are well studied in the game theory literature, we had a rich body of work understanding how rational agents would play, and when linguistic coordination could be useful. 

We focus on two games from within the 2x2 game space: Prisoner's Dilemma (PD) and Bach or Stravinsky (BoS). 

TODO payout table

From game theory analysis, communication should not help in PD as it would be "cheap talk" -- that is, regardless of what the opponent player says, the equilibrium does not change, because there is not an incentive to be truthful. Thus, the prediction from game theory is that among rational agents playing a one-shot game of PD, they will end up in the Nash equilibrium. 

For BoS, there are two pure Nash equilibrium, and thus there is a coordination challenge to agree on which one to go to. In this case, a message from one player saying what they will select will be credible -- having said that, they are incentivized to act on it, and their partner can do better by listening to it. There may be competition over which of the two equilibrium options to agree on, but both players want to go for the same cell. Thus, the prediction is that language will be beneficial in BoS. 

To foreshadow the results, we found that *using* language showed some effect of helping improve results when people used it (including the possibility of cross-round transfer). However, the rate of uptake of the chat interface was low (despite our best efforts), and so we did not find robust effects of condition (chat or no chat). We also cannot know that differences between *those who could use and did use the chat* and other groups were due to the chat use as opposed to selection confounds on who chooses to use the chat. A qualitative look at the language and post-experiment reports reveals various ways overall strategies for achieving fairness (in both conditions) and various strategies for identifying the target each round (when language was used). 

Thus, we still suspect that there probably is something interesting about when language is strategically useful and what sort of procedural and referential language is used. That said, we are unable to provide evidence because over 5 experiments (+ pilots) we continually ran into issues getting sufficient uptake of the condition manipulation, from which we eventually concluded that it was not worth our running additional experiments. 

All data and code is available. 

# Methods
```{r set-up, include=F}
knitr::opts_chunk$set(
  out.width = "\\textwidth",
  fig.align = "center", fig.width = 8, fig.height = 4, fig.crop = F, fig.align = "center", fig.env = "figure",
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)
library(tidyverse)
library(jsonlite)
library(here)
library(rlang)
library(lme4)
library(brms)
library(rstan)
library(viridis)
library(knitr)

theme_set(theme_bw())

source(here("manuscript/prep.R"))

stats <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 1], ": ", model[row, 2], " ", model[row, 3])
}

stats_text <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 2], "  ", model[row, 3])
}

```


## Overall methods

All experiments were implemented in Empirica (v1). We recruited participants via Prolific and directed them to our website where they were paired up to play a game with another participant in real time. 

The experiment consisted first of consent and instructions. Then participants (in all conditions) (TODO confirm which experiments this was true for) were given 3 minutes to freely chat with their partner using the chat box. This was intended to convince participants their partner really was human and not a bot (after high "is a bot" ratings in pilot studies). A few potential prompts for conversation were given. The exact instructions were changed in later experiments after LLM chatbots became common to explicitly tell participants TODO what we actually said. 

Then participants moved onto the main experiment, where on each trial, they saw a payout grid for differently colored treasure chests and were asked to click which chest they wanted to open. Each pair completed 40 trials, with the payoffs dependent on the experiment condition (see below). In some experiments, we showed a running payout score for each participant. (TODO which expts). 

After the experiment, participants completed a survey about their experience, including their strategy and whether they thought they were playing with a real person, before seeing a debrief and being redirected back to prolific to receive payment. 

```{r}
all_rounds |>
  select(gameId, expt, chat_cond) |>
  unique() |>
  group_by(expt, chat_cond) |>
  tally() |>
  mutate(n = 2 * n) |>
  pivot_wider(names_from = chat_cond, values_from = n) |>
  kable(caption = "Number of participants in each experiment and each condition.")
```

## Conditions
For all experiments, we set parameters for the ratios between trial types and the ranges of payout values for each trial type. We used a sampling and randomization approach (see code for details), so different games saw different orders of trials and different values of trials, from the same distributions. 

All experiments did both chat and no-chat versions as a between groups manipulation. 
TODO diagram 
Expt 1 (https://osf.io/8fnze): Between - group manipulation of either all PD trials or all BoS trials. PD trials were created by sampling 3 values from 1-9 (without replacement) for the payoffs, with 0 always as the sucker payoff, with the preference order corresponding to PD structure. For BoS, the off-diagonal payoff was always 1, the other payoffs were sampled from 2-9 (without replacement). 

TODO confirm payoffs!


Expt 2 (https://osf.io/5au2r): A within-group version of experiment 1. Each game had roughly half and half PD and BoS trials, with trials constructed in the same way as experiment 1.

During data analysis for experiment 2, we realized that we had been misdisplaying the off diagonal rewards for PD for one player in each game. This was fixed for subsequent experiments.

Expt 3 (https://osf.io/c274z): Each game had a mix of two different tyes of PD and two different types of BoS, so that overall roughly half of trials were BoS and half were PD. Of the 40 trials, exactly 4, at pre-set locations in the trial order were "spiked" BoS where one of the rewards was high (sampled from 25-30) and the other was normal (sampled from 3-7). Due to the higher stakes, we thought pairs might coordinate more on these trials. We also included roughly 16 "normal" BoS trials with payouts sampled as in Experiments 1 and 2. 

When randomly sampling numbers to populate PD trials, generally, the welfare maximizing option (highest summed reward) comes from both cooperating. Sometimes, however the payout for defecting is greater than twice the both-cooperate value and so the welfare maximizing option is to agree to defect-cooperate. Thus, in this situation, one might be able to cooperate for an overall better outcome than could otherwise be achieved, so we aimed to see whether people would recognize this type. 

Of the roughly 20 trials in experiment 3 that were PD type, half were "easy" PD trials where where cooperating is welfare maximizing ( 2 * coop payoff greater than or equal to defect payoff). All rewards are chosen from [1-12] conditional on this distribution. The "sucker" payoff was 0. The other half of PD trials (roughly 1/4 of the overall game) were "hard" PD trials where having one person defect and the other take the sucker payoff is welfare maximizing (the defect payoff is > 2 * coop payoff). All rewards are chosen from [1-12] conditional on this constraint. 

Expt 4 (): In experiment 4, we tried to get a set-up that was closer to the predictions from game theory. We switched from providing a bonus proportional to the overall points earned to an incentive compatible bonus that sampled TODO N trials at random and paid out those points. We also removed the points counter, given that participants previously reported using the visible money earned so far as a coordination locus. TODO

Expt 5 (): The condition distribution of experiment 5 was identical to experiment 3.  TODO

## Data processing
All data processing was done in R, the code is available. TODO

# Results

the goal was to look at how people might use langauge to coordinate and thus achieve better outcomes than they could without language. We were also interested in what sort of coordination language might be used, and whether procedural or meta-conventions might occur. 

In terms of outcomes, we can look at points earned (which includes the random noise from what the points spreads were). We can also look at what outcome quadrant participants are selecting. 

One issue that arose was that we had some difficulty convincing participants their partners were indeed real humans, and substantial difficulty creating a situation where people in the chat condition would choose to use the chat. 

We also do analysis breaking out our chat condition into whether or not people chose to use the chat. However, this breaks the randomization and introduces as confounds any exogenous factor (other than access to chat) that determines whether people chat (i.e. contentiousness, motivation, etc). 

## Manipulation results

The interpretability of results depends on two factors: a) whether participants generally think they are playing with another human and b) whether chat use actually differs substantially by condition. 


```{r bot}
exit_1 <- read_csv(here(study_1_loc, "exit.csv")) |>
  group_by(game_cond, chat_cond, human) %>%
  tally() %>%
  pivot_wider(names_from = human, values_from = n) %>%
  mutate(pct = yes / (no + yes)) |>
  mutate(expt = str_c("1", "_", game_cond)) |>
  ungroup()

exit_2 <- read_csv(here(study_2_loc, "exit.csv")) |>
  group_by(chat_cond, human) %>%
  tally() %>%
  pivot_wider(names_from = human, values_from = n) %>%
  mutate(pct = yes / (no + yes)) |>
  mutate(expt = "2")

exit_3 <- read_csv(here(study_3_loc, "exit.csv")) |>
  group_by(chat_cond, human) %>%
  tally() %>%
  pivot_wider(names_from = human, values_from = n) %>%
  mutate(pct = yes / (no + yes)) |>
  mutate(expt = "3")

exit_4 <- read_csv(here(study_4_loc, "exit.csv")) |>
  group_by(chat_cond, human) %>%
  tally() %>%
  pivot_wider(names_from = human, values_from = n) %>%
  mutate(pct = yes / (no + yes)) |>
  mutate(expt = "4")

exit_5 <- read_csv(here(study_5_loc, "exit.csv")) |>
  group_by(chat_cond, human) %>%
  tally() %>%
  pivot_wider(names_from = human, values_from = n) %>%
  mutate(pct = yes / (no + yes)) |>
  mutate(expt = "5")

exit <- exit_1 |>
  bind_rows(exit_2) |>
  bind_rows(exit_3) |>
  bind_rows(exit_4) |>
  bind_rows(exit_5) |>
  select(expt, chat_cond, pct) |>
  mutate(chat_cond = ifelse(chat_cond == "chat", "chat", "nochat"))

exit |>
  mutate(pct = round(pct * 100, 1)) |>
  pivot_wider(names_from = chat_cond, values_from = pct) |>
  kable(caption = "Percent of participants in each condition who thought their partner was human as reported in an exit survey.")
```

For the most part, we succeeded in convincing players their partner was indeed human. Rates of believing partner was human were slightly higher in the chat conditions than no-chat conditions. 

Our other manipulation check was how well participants took up the manipulation of being able to use a chat to communicate with their partner. 

As shown in Figure TODO, groups vary substantially in how many trials they use the chat on, but many groups rarely use the chat. The horizontal lines correspond to 10% and 50% of the time, so games lower than the 10% line are using chat on fewer than 4 (out of 40) trials. There's a lot of not chatting in the access-to-chat condition.  


```{r chat-pct, fig.width=10, fig.height=3, }

game_chat <- all_chat %>%
  filter(!is.na(targets)) %>%
  select(-type) %>%
  full_join(all_rounds) %>%
  mutate(words = str_count(text, "\\W+") %|% int(0)) |>
  group_by(gameId, gametype, cond, chat_cond, repNum, expt) %>%
  summarize(
    words = sum(words),
    mean_payout = mean(payoff),
    is_chat = ifelse(words > 0, 1, 0)
  )

game_chat |>
  filter(chat_cond == "chat") |>
  mutate(is.chat = ifelse(words > 0, 1, 0)) %>%
  group_by(gameId, expt) %>%
  summarize(pct_chat = mean(is.chat)) |>
  arrange(expt, pct_chat) |>
  group_by(expt) |>
  mutate(x = 1 / n() * row_number()) |>
  ggplot(aes(x = x, y = pct_chat)) +
  geom_point() +
  facet_grid(.~expt) +
  coord_cartesian(ylim = c(-.05, 1.05),xlim=c(-.05, 1.05), expand=F) +
  geom_hline(yintercept = .1) +
  geom_hline(yintercept = .5) +
  labs(x="Fraction games", y="Fraction trials with chat messages")
```

## Main results

The prediction is that having access to the communication modality will be helpful for aligning coordination in BoS where both players want to choose the same thing. 

Coordination should be less helpful in PD. According to theory, for single-shot, rational actors should choose the nash equilibrium. Participants are nicer than that and instead coordinate on the unstable mutually beneficial equilibrium. 

After observing that occasionally there were PD trials were the welfare-maximizing option was for one person to get a large payout and the other to get nothing (called "hard" or "sacrifice" PD trials in expt 3 onwards), we explicitly sampled both this "hard" structure where the best overall outcome is to coordinate on one person getting the big reward and balance out later by switching off. 

In contrast, we refer to the trials in which only one option is welfare maximizing as "easy PD" where we do not expect language to help. 

The first way we assess whether access to chat makes a difference on game performance is to look at the mean number of points recieved per trial. Because BoS and PD have different point structures, we compare within game type. 

```{r points_pd, fig.cap="TODO fig cap"}
bonuses <- all_rounds %>%
  group_by(playerId, gameId, chat_cond, cond, gametype, expt) %>%
  summarize(payoff = mean(payoff))



ggplot(bonuses, aes(x = chat_cond, y = payoff, color = str_c(cond, "_", gametype), group = cond)) +
  geom_point(position = position_jitterdodge(jitter.width = .1, dodge.width = .7), alpha = .3) +
  facet_grid(gametype~expt) +
  scale_color_brewer(palette = "Dark2") +
  labs(y = "mean_reward", x = "condition") +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .7), size = .7, color = "black") +
    stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .7), size = .7, color = "black", geom="line") +

  theme(legend.position = "bottom") +
  coord_cartesian(ylim = c(0, 20)) +
  labs(y = "Mean reward / trial", ) +
  theme(
    legend.title=element_blank(),
    axis.title.x=element_blank(),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    strip.text = element_text(size = 14)
  )
```

TODO set per-condition color scheme!

In BoS, there is a generally a slightly higher score in the chat condition than the nochat condition, whereas there is not a difference between condtions in PD. 

Looking at points introduces some noise as the points were varying even within a type of game. To understand more what is being selected, we can look at what quadrant games are ending up in.

TODO include topline model results

### Quadrant analysis per trial

```{r}
outcome <- all_rounds %>%
  select(gameId, repNum, response, chat_cond, gametype, cond, role) |>
  pivot_wider(names_from = role, values_from = response) %>%
  mutate(outcome = str_c(p1, p2)) %>%
  left_join(game_chat)
```

For BoS, the desirable outcome is to both choose the same box. As can be seen in Figure TODO, this good outcome is achieved at above change levels when people can chat and usually at chance levels when people cannot. 

```{r, fig.cap="In BoS: P1 prefers AA to BB, P2 prefers BB to AA. AB and BA are 0 for both. "}
outcome %>%
  filter(gametype == "BoS") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("AA", "BB"), 1, 0)) %>%
  ggplot(aes(x = str_c(chat_cond), y = outcome_parity, color=cond, group=cond)) +
  facet_grid(~ expt) +
  stat_summary(fun.data = "mean_cl_boot", position=position_dodge(width=.3)) +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "BoS:fraction choosing good outcomes (AA or BB)", y = "Good outcome?")+
  theme(legend.position="bottom")
```
We again note that the off-diagonal rewards were misdisplayed for one of the players in experiments 1 and 2 for PD. On most trials "normal_PD" will pattern with easy PD. 

In easyPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. AA is welfare maximizing.

In hardPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. BA and AB are welfare maximizing.


```{r}
outcome %>%
  filter(gametype == "PD") %>%
  mutate(AA = ifelse(outcome %in% c("AA"), 1, 0)) %>%
  mutate(AB_BA= ifelse(outcome %in% c("AB", "BA"), 1, 0)) %>%
  mutate(BB = ifelse(outcome %in% c("BB"), 1, 0)) %>%
  pivot_longer(c("AA", "AB_BA", "BB"), names_to="outcome_type", values_to="selected") |> 
  ggplot(aes(x = chat_cond, y = selected, color=cond)) +
  facet_grid(outcome_type ~ expt) +
  stat_summary(fun.data = "mean_cl_boot", position=position_dodge(width=.3)) +
  geom_hline(aes(yintercept = .5)) +
  theme(legend.position="bottom")
```
From Figure TODO, we see that the preferred outcome is AA which is welfare-maximizing in easy PD. This is not what game theory predicts for rational actors, instead, participants are being nice and cooperative with each other. 

There is a slight tendency for more AA selections in easy than hard PD, especially in the chat conditions, but slight. 


# Analyses based on use of chat 

## Per game does talking help?

One idea is that talking on some trials may set up coordination strategies that can then effectively be used on later trials without talking on those trials. 

So we want to look at overall volume of talking (in words or in # of trial talked) as a predictor for performance, controlling for talk on that round? 

```{r}
group_talk <- game_chat |>
  group_by(gameId) |>
  summarize(total_words = sum(words), pct_chat = mean(is_chat))

overall_talk <- game_chat |> left_join(group_talk)
ggplot(overall_talk |> filter(gametype == "BoS", chat_cond == "chat"), aes(x = pct_chat, y = mean_payout, color = as.character(is_chat))) +
  # stat_summary(fun.data="mean_cl_boot")+
  # geom_point()+
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")

ggplot(overall_talk |> filter(gametype == "PD", chat_cond == "chat"), aes(x = pct_chat, y = mean_payout, color = as.character(is_chat))) +
  # stat_summary(fun.data="mean_cl_boot")+
  # geom_point()+
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")
```

So, in mixed games, talking on more trials helps regardess of whether you talked on this particular trial. (Although overall talk might also be confounding with conscientiousness..., to fix you'd have to experimentally manipulate when chat is or isn't available)

Talking more might help with hard PD, maybe in mixed?


```{r}
ggplot(overall_talk |> filter(gametype == "BoS", total_words > 0), aes(x = log(total_words), y = mean_payout, color = as.character(is_chat))) +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")

ggplot(overall_talk |> filter(gametype == "PD", total_words > 0), aes(x = log(total_words), y = mean_payout, color = as.character(is_chat))) +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")
```

Here total words is going to mix how much you talk each time with how often you talk, so maybe this is just totally redundant with previous. More talk helps on BoS, even as transfer from other trials. PD seems messier? 






## Quadrant analysis per trial

 - chat_0 is had access to chat and didn't use on that trial
 - chat_1 is used chat on that trial
 - nochat_0 did not have access to chat 
 


```{r}
outcome <- all_rounds %>%
  select(gameId, repNum, response, chat_cond, gametype, cond, role) |>
  pivot_wider(names_from = role, values_from = response) %>%
  mutate(outcome = str_c(p1, p2)) %>%
  left_join(game_chat)
```

## BoS

In BoS: P1 prefers AA to BB, P2 prefers BB to AA. AB and BA are 0 for both. 

```{r}
outcome %>%
  filter(gametype == "BoS") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("AA", "BB"), 1, 0)) %>%
  ggplot(aes(x = str_c(chat_cond, "_", is_chat), y = outcome_parity)) +
  facet_grid(. ~ cond) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "BoS:fraction choosing good outcomes (AA or BB)", y = "Good outcome?")
```

Near chance if you can't talk, above chance if you could but don't,  far above chance if you do. 

```{r}
outcome %>%
  filter(gametype == "BoS") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("AA", "BB"), 1, 0)) %>%
  ggplot(aes(x = repNum, y = outcome_parity, color = cond)) +
  facet_grid(~ str_c(chat_cond, "_", is_chat)) +
  geom_smooth(method = "lm", se = F) +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "BoS:fraction choosing good outcomes", y = "Good outcome?")
```

## PD

Expts 1 and 2 are suspect, but on most trials will pattern with easy PD. 

In easyPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. AA is welfare maximizing.

In hardPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. BA and AB are welfare maximizing.


```{r}
outcome %>%
  filter(gametype == "PD") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("AA"), 1, 0)) %>%
  ggplot(aes(x = str_c(chat_cond, "_", is_chat), y = outcome_parity)) +
  facet_grid(. ~ cond) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "PD:fraction choosing AA outcome")

outcome %>%
  filter(gametype == "PD") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("AB", "BA"), 1, 0)) %>%
  ggplot(aes(x = str_c(chat_cond, "_", is_chat), y = outcome_parity)) +
  facet_grid(. ~ cond) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "PD:fraction choosing AB or BA outcome")

outcome %>%
  filter(gametype == "PD") %>%
  mutate(outcome_parity = ifelse(outcome %in% c("BB"), 1, 0)) %>%
  ggplot(aes(x = str_c(chat_cond, "_", is_chat), y = outcome_parity)) +
  facet_grid(. ~ cond) +
  stat_summary(fun.data = "mean_cl_boot") +
  geom_hline(aes(yintercept = .5)) +
  labs(title = "PD:fraction choosing BB outcome")
```

Can get a reasonable option no matter what. If you talk, you can get the uneven, but welfare-maximizing in hard PD. 



Looks like using language tends to help, on the trials it's used on, and slightly on the trials it's not used on for BoS. Helps in some cases for PD. 

Is there a dose-response relationship, or is one word enough?

```{r}
ggplot(game_chat |> filter(gametype == "BoS", words > 0), aes(x = log(words), y = mean_payout)) +
  geom_point() +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")

ggplot(game_chat |> filter(gametype == "PD", words > 0), aes(x = log(words), y = mean_payout)) +
  geom_point() +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")
```

Especially where we have more data, looks like one word is enough. Indicative of coordination rather than negotiation, probably? 


## Per game does talking help?

One idea is that talking on some trials may set up coordination strategies that can then effectively be used on later trials without talking on those trials. 

So we want to look at overall volume of talking (in words or in # of trial talked) as a predictor for performance, controlling for talk on that round? 

```{r}
group_talk <- game_chat |>
  group_by(gameId) |>
  summarize(total_words = sum(words), pct_chat = mean(is_chat))

overall_talk <- game_chat |> left_join(group_talk)
ggplot(overall_talk |> filter(gametype == "BoS", chat_cond == "chat"), aes(x = pct_chat, y = mean_payout, color = as.character(is_chat))) +
  # stat_summary(fun.data="mean_cl_boot")+
  # geom_point()+
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")

ggplot(overall_talk |> filter(gametype == "PD", chat_cond == "chat"), aes(x = pct_chat, y = mean_payout, color = as.character(is_chat))) +
  # stat_summary(fun.data="mean_cl_boot")+
  # geom_point()+
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")
```

So, in mixed games, talking on more trials helps regardess of whether you talked on this particular trial. (Although overall talk might also be confounding with conscientiousness..., to fix you'd have to experimentally manipulate when chat is or isn't available)

Talking more might help with hard PD, maybe in mixed?


```{r}
ggplot(overall_talk |> filter(gametype == "BoS", total_words > 0), aes(x = log(total_words), y = mean_payout, color = as.character(is_chat))) +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")

ggplot(overall_talk |> filter(gametype == "PD", total_words > 0), aes(x = log(total_words), y = mean_payout, color = as.character(is_chat))) +
  facet_grid(gametype ~ cond) +
  geom_smooth(method = "lm")
```

Here total words is going to mix how much you talk each time with how often you talk, so maybe this is just totally redundant with previous. More talk helps on BoS, even as transfer from other trials. PD seems messier? 




# Language results: what do people say
and what does this say about coordination strategies

## Language 

How much language? 

Filter only for games that talked at least a little. 

Second graph filters for *trials* that talked. 

```{r}
game_chat |>
  group_by(gameId) |>
  summarize(total_words = sum(words)) |>
  filter(total_words != 0) |>
  left_join(game_chat) |>
  mutate(expt = str_sub(cond, -1)) |>
  ggplot(aes(x = repNum, y = words)) +
  geom_point() +
  facet_wrap(~expt) +
  geom_smooth()

game_chat |>
  group_by(gameId) |>
  summarize(total_words = sum(words)) |>
  filter(total_words != 0) |>
  left_join(game_chat) |>
  filter(words > 0) |>
  mutate(expt = str_sub(cond, -1)) |>
  ggplot(aes(x = repNum, y = words)) +
  geom_point() +
  facet_wrap(~expt) +
  geom_smooth()
```


## how often do *both* people talk

```{r}
pair_chat <- all_chat %>%
  filter(!is.na(targets)) %>%
  select(-type) %>%
  full_join(all_rounds) %>%
  mutate(words = str_count(text, "\\W+") %|% int(0)) |>
  group_by(gameId, playerId, gametype, cond, chat_cond, repNum) %>%
  summarize(
    words = sum(words),
    is_chat = ifelse(words > 0, 1, 0)
  ) |>
  group_by(gameId, gametype, cond, chat_cond, repNum) |>
  summarize(how_many_chat = sum(is_chat)) |>
  group_by(gameId, gametype, cond, chat_cond) |>
  summarize(pct_both = mean(how_many_chat == 2)) |>
  filter(chat_cond == "chat")

ggplot(pair_chat, aes(x = cond, y = pct_both)) +
  geom_jitter(width = .2) +
  facet_wrap(~gametype)
```
Even in games that talk, there aren't *that* many trials where both people talk? 

Probably going to need to manually look at the language for negotiation v "choose red" "yup" etc. 


# Discussion 
Some caveats: Despite our desire to use game-theory aligned paradigms for the analytic theory and predictions, the experiments had several factors that cause potential theoretical misalignment. 

1. For practical reasons, we ran repeated trials with the same partners. This iterated approach is also studied, but potentially changes what exactly the predictions are. 

2. The incentives of participants are not the same as the incentives of the "agents". People have values above money such as their reputation and desire to be nice to other people. Additionally, the monetary value participants get from the task is not what is shown in the boxes. They were paid a base rate for participation + a bonus aligned to task performance. Strategies of random clicking could be cost efficient if it means the task takes sufficiently shorter that base rate / short time is a better wage than (base rate + higher bonus)/ longer time. 

3. 


# Appendix: Model Results

Here we present the statistical results we ran. Note that these are not all the statistics we ever ran and may not be the pre-registered ones. These are what I thought best summarized the data descriptively and addressed the hypotheses at the time this summary was written. 

```{r}
form <- function(model) {
  dep <- as.character(model$formula[2])
  ind <- as.character(model$formula[3])

  str_c(dep, " ~ ", ind) |>
    str_replace_all(" ", "") |>
    str_replace_all("~", "~$\\\\sim$ ") |>
    str_replace_all("\\*", "~$\\\\times$ ") |>
    str_replace_all("\\+", "~+ ") |>
    str_replace_all("_", "")
}

do_table <- function(mod, cap, decimal = 2) {
  model <- read_rds(here(msum_loc, mod)) |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Term = str_replace_all(Term, "_", ""),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)

  spec <- read_rds(here(mform_loc, mod))

  kable(model |> select(Term, Est. = Estimate, `95\\% CrI` = `Credible Interval`),
    escape = F, format = "latex", booktabs = T, linesep = "",
    caption = str_c(cap, ": ", form(spec), sep = ""),
    align = "lll", position="H"
  )
}

msum_loc="manuscript/model_files/summary"
mform_loc="manuscript/model_files/formulae"
```

```{r}
do_table("payout_mega.rds", cap="TODO")
do_table("payout_mega_combo.rds", cap="TODO")
do_table("payout_combo_3.rds", cap="TODO")
do_table("payout_combo_4.rds", cap="TODO")
do_table("payout_combo_5.rds", cap="TODO")

do_table("BoS_aligned_mega.rds", cap="TODO")
do_table("BoS_aligned_3.rds", cap="TODO")
do_table("BoS_aligned_4.rds", cap="TODO")
do_table("BoS_aligned_5.rds", cap="TODO")

do_table("PD_cooperate_mega.rds", cap="TODO")
do_table("PD_cooperate_3.rds", cap="TODO")
do_table("PD_cooperate_4.rds", cap="TODO")
do_table("PD_cooperate_5.rds", cap="TODO")


do_table("PD_unneven_mega.rds", cap="TODO")
do_table("PD_unneven_3.rds", cap="TODO")
do_table("PD_unneven_4.rds", cap="TODO")
do_table("PD_unneven_5.rds", cap="TODO")


do_table("PD_nash_mega.rds", cap="TODO")
do_table("PD_nash_3.rds", cap="TODO")
do_table("PD_nash_4.rds", cap="TODO")
do_table("PD_nash_5.rds", cap="TODO")



```

 [1] "BoS_aligned_3.rds"     "BoS_aligned_4.rds"     "BoS_aligned_5.rds"    
 [4] "BoS_aligned_mega.rds"  "payout_mega_3.rds"     "payout_mega_4.rds"    
 [7] "payout_mega_combo.rds" "payout_mega.rds"       "PD_cooperate_3.rds"   
[10] "PD_cooperate_4.rds"    "PD_cooperate_5.rds"    "PD_cooperate_mega.rds"
[13] "PD_nash_3.rds"         "PD_nash_4.rds"         "PD_nash_5.rds"        
[16] "PD_nash_mega.rds"      "PD_unneven_3.rds"      "PD_unneven_4.rds"     
[19] "PD_unneven_5.rds"      "PD_unneven_mega.rds"  

## payout models

First up, payout models. 

## BoS alignment models


## PD alignment models

In easyPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. AA is welfare maximizing.

In hardPD: P1 prefers BA > AA > BB > AB and P2 prefers AB > AA > BB > BA. BA and AB are welfare maximizing.


TODO run Bayesian
TODO save Bayesian
TODO tables 

# Random crap



## Condition assignment on performance

```{r, eval=F}
perf_priors <- c(
  set_prior("normal(5,2)", class = "Intercept"),
  set_prior("normal(0, 2)", class = "b"),
  set_prior("normal(0, 2)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

perf_mod <- brm(
  payoff ~ has_chat * is_BoS + repNum +
    (is_BoS | gameId) +
    (is_BoS | playerId),
  data = for_mod,
  prior = perf_priors,
  control = list(adapt_delta = .95)
)

perf_mod_tot <- brm(
  payoff ~ (use_chat | has_chat) * is_BoS +
    repNum +
    (is_BoS | gameId) +
    (is_BoS | playerId),
  data = for_mod,
  prior = perf_priors,
  control = list(adapt_delta = .95)
)

freq_tot <- lm(payoff ~ (use_chat | has_chat) * is_BoS +
  repNum, data = for_mod)
```

## Words over time 

log(words) ~ game_type * trial_number

```{r, eval=F}
for_log_words <- for_mod |>
  filter(words > 0) |>
  mutate(log_words = log(words))

lang_priors <- c(
  set_prior("normal(5,5)", class = "Intercept"),
  set_prior("normal(0, 5)", class = "b"),
  set_prior("normal(0, 5)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

log_words_mod <- brm(
  log_words ~ is_BoS * repNum +
    (is_BoS * repNum | gameId),
  data = for_log_words,
  prior = lang_priors,
  control = list(adapt_delta = .95)
)
```

## Words on outcome

outcome ~ game_type * log(words)

```{r, eval=F}
perf_word_mod <- brm(
  payoff ~ log_words * is_BoS + repNum +
    (is_BoS | gameId) +
    (is_BoS | playerId),
  data = for_log_words,
  prior = perf_priors,
  control = list(adapt_delta = .95)
)
```


```{r, eval=F}
ggplot(for_log_words, aes(x = log_words, y = payoff, color = repNum)) +
  geom_jitter(alpha = .5) +
  geom_smooth(method = "lm") +
  facet_wrap(is_BoS ~ cond)
```

TODO think more about!!
outcome ~ log(words_first_10)*game_type*words_now ? 

```{r, eval=F}
early_words <- for_mod |>
  filter(repNum < 10) |>
  group_by(gameId) |>
  summarize(early_words = sum(words))

for_early_word_mod <- for_mod |>
  filter(repNum > 9) |>
  left_join(early_words) |>
  filter(early_words > 0) |>
  mutate(log_early_words = log(early_words))

early_word_mod <- brm(
  payoff ~ log_early_words * is_BoS * is_chat + repNum +
    (is_BoS | gameId) +
    (is_BoS | playerId),
  data = for_early_word_mod,
  prior = perf_priors,
  control = list(adapt_delta = .95)
)
```
-->
